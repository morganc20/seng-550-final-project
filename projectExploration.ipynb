{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608778a8",
   "metadata": {},
   "outputs": [],
   "source": "# %pip install boto3 python-dotenv pyspark torch transformers scikit-learn matplotlib --user"
  },
  {
   "cell_type": "code",
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:25:07.823977Z",
     "start_time": "2024-12-19T04:25:07.077719Z"
    }
   },
   "source": [
    "import boto3\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def load_jsonl_from_s3(bucket_name, file_key):\n",
    "    \"\"\"\n",
    "    Load a JSONL file from an S3 bucket using credentials from environment variables.\n",
    "\n",
    "    Parameters:\n",
    "    - bucket_name: str - Name of the S3 bucket.\n",
    "    - file_key: str - Key (path) of the JSONL file in the bucket.\n",
    "\n",
    "    Returns:\n",
    "    - List of Python dictionaries loaded from the JSONL file.\n",
    "    \"\"\"\n",
    "    # Get AWS credentials from environment variables\n",
    "    aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "    aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    region_name = os.getenv(\"AWS_REGION\")\n",
    "    \n",
    "    # Initialize an S3 client\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "        region_name=region_name\n",
    "    )\n",
    "    \n",
    "    # Retrieve the file content\n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    # Parse the JSONL content\n",
    "    data = [json.loads(line) for line in content.splitlines() if line.strip()]\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    bucket_name = \"small-reviews584\"\n",
    "    file_key = \"data/reviews_small.jsonl\"\n",
    "    \n",
    "    data = load_jsonl_from_s3(bucket_name, file_key)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b07955263870209d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:31:51.877741Z",
     "start_time": "2024-12-19T04:31:45.315Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "#import pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AmazonReviewsLocalLR\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Convert Python list of dicts to Spark DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Ensure the expected columns exist: 'text' and 'rating'\n",
    "required_columns = [\"text\", \"rating\"]\n",
    "for col_name in required_columns:\n",
    "    if col_name not in df.columns:\n",
    "        raise ValueError(f\"Expected column '{col_name}' not found in data\")\n",
    "\n",
    "# Create binary label: label=1 if rating >=3 else 0\n",
    "df = df.withColumn(\"label\", when(col(\"rating\") >= 3, 1).otherwise(0))\n",
    "\n",
    "# Filter out rows without text\n",
    "df = df.filter(col(\"text\").isNotNull())\n",
    "\n",
    "# Define text processing and feature extraction pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=20, regParam=0.001)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Fit the model\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Evaluate model performance\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = binary_evaluator.evaluate(predictions)\n",
    "\n",
    "# For multi-metric evaluation\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = multi_evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "precision = multi_evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
    "recall = multi_evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
    "f1 = multi_evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
    "\n",
    "print(\"=== Performance Metrics ===\")\n",
    "print(f\"Test AUC (Area Under ROC): {auc:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1-score: {f1:.4f}\")\n",
    "\n",
    "# # Show a confusion matrix-like table\n",
    "# predictions.groupBy(\"label\", \"prediction\").count().show()\n",
    "\n",
    "# Stop the Spark session (if you do not need it further)\n",
    "# spark.stop()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Performance Metrics ===\n",
      "Test AUC (Area Under ROC): 0.7345\n",
      "Test Accuracy: 0.8333\n",
      "Test Precision: 0.8372\n",
      "Test Recall: 0.8333\n",
      "Test F1-score: 0.8352\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "69708857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:31:34.663793Z",
     "start_time": "2024-12-19T04:25:46.338251Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.nn import Module, Linear, Sigmoid\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Dataset Definition\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, tokenizer, max_length=128):\n",
    "        self.reviews = reviews\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        text = review[\"title\"] + \" \" + review[\"text\"]\n",
    "        rating = review[\"rating\"]\n",
    "        label = 1 if rating >= 4 else 0\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "# Model Definition\n",
    "class ReviewClassifier(Module):\n",
    "    def __init__(self, pretrained_model_name=\"bert-base-uncased\", hidden_dim=128):\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.fc1 = Linear(self.bert.config.hidden_size, hidden_dim)\n",
    "        self.fc2 = Linear(hidden_dim, 1)\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = bert_output.pooler_output\n",
    "        x = F.relu(self.fc1(pooled_output))\n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# Metrics Evaluation\n",
    "def evaluate_metrics(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask).squeeze()\n",
    "            preds = (outputs > 0.5).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Training Loop\n",
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Initialize tokenizer and dataset\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "max_length = 128\n",
    "dataset = ReviewDataset(data, tokenizer, max_length=max_length)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ReviewClassifier().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training the model\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating on the test set...\")\n",
    "accuracy, precision, recall, f1 = evaluate_metrics(model, test_loader, device)\n",
    "\n",
    "print(f\"Test Metrics - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"review_classifier.pt\")\n",
    "print(\"Model saved to review_classifier.pt\")\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d93ba31bf9e4f01a83637a52ff941ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 0.5004\n",
      "Epoch 2/3, Train Loss: 0.2962\n",
      "Epoch 3/3, Train Loss: 0.1739\n",
      "Evaluating on the test set...\n",
      "Test Metrics - Accuracy: 0.9300, Precision: 0.9593, Recall: 0.9593, F1 Score: 0.9593\n",
      "Model saved to review_classifier.pt\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "b672b309",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:32:36.032587Z",
     "start_time": "2024-12-19T04:32:24.424682Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SentimentClassifier\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create a binary label: favorable (1) for ratings >= 3, not favorable (0) otherwise\n",
    "df = df.withColumn(\"label\", when(col(\"rating\") >= 3, 1).otherwise(0))\n",
    "\n",
    "# Process text column\n",
    "text_tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"text_words\")\n",
    "text_remover = StopWordsRemover(inputCol=\"text_words\", outputCol=\"filtered_text_words\")\n",
    "text_hashingTF = HashingTF(inputCol=\"filtered_text_words\", outputCol=\"text_rawFeatures\", numFeatures=10000)\n",
    "text_idf = IDF(inputCol=\"text_rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Process title column\n",
    "title_tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"title_words\")\n",
    "title_remover = StopWordsRemover(inputCol=\"title_words\", outputCol=\"filtered_title_words\")\n",
    "title_hashingTF = HashingTF(inputCol=\"filtered_title_words\", outputCol=\"title_rawFeatures\", numFeatures=5000)\n",
    "title_idf = IDF(inputCol=\"title_rawFeatures\", outputCol=\"title_features\")\n",
    "\n",
    "# Combine features from text and title\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=[\"features\", \"title_features\"], \n",
    "    outputCol=\"assembled_features\"\n",
    ")\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"assembled_features\", labelCol=\"label\", numTrees=100, maxDepth=10, seed=42)\n",
    "\n",
    "# Pipeline to chain all the stages together\n",
    "rf_pipeline = Pipeline(stages=[\n",
    "    text_tokenizer, text_remover, text_hashingTF, text_idf,\n",
    "    title_tokenizer, title_remover, title_hashingTF, title_idf,\n",
    "    feature_assembler, rf\n",
    "])\n",
    "\n",
    "# Train/test data split\n",
    "rf_train_df, rf_test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model = rf_pipeline.fit(rf_train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_predictions = rf_model.transform(rf_test_df)\n",
    "\n",
    "# Evaluate model performance and metrics\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = binary_evaluator.evaluate(rf_predictions)\n",
    "print(f\"Test AUC: {auc}\")\n",
    "\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "accuracy = multi_evaluator.evaluate(rf_predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
    "precision = multi_evaluator.evaluate(rf_predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = multi_evaluator.evaluate(rf_predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = multi_evaluator.evaluate(rf_predictions, {multi_evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Precision: {precision}\")\n",
    "print(f\"Test Recall: {recall}\")\n",
    "print(f\"Test F1 Score: {f1}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "# spark.stop()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.702440458688621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9040404040404041\n",
      "Test Precision: 0.817289052137537\n",
      "Test Recall: 0.9040404040404041\n",
      "Test F1 Score: 0.8584786860648931\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "fcb0cb21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:32:50.622639Z",
     "start_time": "2024-12-19T04:32:44.256509Z"
    }
   },
   "source": [
    "#naive bayes\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, VectorAssembler\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SentimentClassifier\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create a binary label: favorable (1) for ratings >= 3, not favorable (0) otherwise\n",
    "df = df.withColumn(\"label\", when(col(\"rating\") >= 3, 1).otherwise(0))\n",
    "\n",
    "# Process text column\n",
    "text_tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"text_words\")\n",
    "text_remover = StopWordsRemover(inputCol=\"text_words\", outputCol=\"filtered_text_words\")\n",
    "text_hashingTF = HashingTF(inputCol=\"filtered_text_words\", outputCol=\"text_rawFeatures\", numFeatures=10000)\n",
    "text_idf = IDF(inputCol=\"text_rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Process title column\n",
    "title_tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"title_words\")\n",
    "title_remover = StopWordsRemover(inputCol=\"title_words\", outputCol=\"filtered_title_words\")\n",
    "title_hashingTF = HashingTF(inputCol=\"filtered_title_words\", outputCol=\"title_rawFeatures\", numFeatures=5000)\n",
    "title_idf = IDF(inputCol=\"title_rawFeatures\", outputCol=\"title_features\")\n",
    "\n",
    "# Combine features from text and title\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=[\"features\", \"title_features\"], \n",
    "    outputCol=\"assembled_features\"\n",
    ")\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "nb = NaiveBayes(featuresCol=\"assembled_features\", labelCol=\"label\", smoothing=1.0)\n",
    "\n",
    "# Pipeline to chain all the stages together\n",
    "nb_pipeline = Pipeline(stages=[\n",
    "    text_tokenizer, text_remover, text_hashingTF, text_idf,\n",
    "    title_tokenizer, title_remover, title_hashingTF, title_idf,\n",
    "    feature_assembler, nb\n",
    "])\n",
    "\n",
    "# Train/test data split\n",
    "nb_train_df, nb_test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "nb_model = nb_pipeline.fit(nb_train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "nb_predictions = nb_model.transform(nb_test_df)\n",
    "\n",
    "# Evaluate model performance and metrics\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "auc = binary_evaluator.evaluate(nb_predictions)\n",
    "\n",
    "print(f\"Test AUC: {auc}\")\n",
    "\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = multi_evaluator.evaluate(nb_predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
    "precision = multi_evaluator.evaluate(nb_predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = multi_evaluator.evaluate(nb_predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = multi_evaluator.evaluate(nb_predictions, {multi_evaluator.metricName: \"f1\"})\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Precision: {precision}\")\n",
    "print(f\"Test Recall: {recall}\")\n",
    "print(f\"Test F1 Score: {f1}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop() # Commented out to avoid stopping the Spark session before using it for the next example, add to last algo cell\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.49456042340488093\n",
      "Test Accuracy: 0.8434343434343434\n",
      "Test Precision: 0.879016354016354\n",
      "Test Recall: 0.8434343434343434\n",
      "Test F1 Score: 0.8585264513630095\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2c015e83c02ca4b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
