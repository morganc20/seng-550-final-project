{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:31:50.626881Z",
     "start_time": "2024-12-16T22:31:49.945244Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def load_jsonl_from_s3(bucket_name, file_key):\n",
    "    \"\"\"\n",
    "    Load a JSONL file from an S3 bucket using credentials from environment variables.\n",
    "\n",
    "    Parameters:\n",
    "    - bucket_name: str - Name of the S3 bucket.\n",
    "    - file_key: str - Key (path) of the JSONL file in the bucket.\n",
    "\n",
    "    Returns:\n",
    "    - List of Python dictionaries loaded from the JSONL file.\n",
    "    \"\"\"\n",
    "    # Get AWS credentials from environment variables\n",
    "    aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "    aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    region_name = os.getenv(\"AWS_REGION\")\n",
    "    \n",
    "    # Initialize an S3 client\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "        region_name=region_name\n",
    "    )\n",
    "    \n",
    "    # Retrieve the file content\n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    # Parse the JSONL content\n",
    "    data = [json.loads(line) for line in content.splitlines() if line.strip()]\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    bucket_name = \"small-reviews584\"\n",
    "    file_key = \"data/reviews_small.jsonl\"\n",
    "    \n",
    "    data = load_jsonl_from_s3(bucket_name, file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b07955263870209d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T22:37:26.167585Z",
     "start_time": "2024-12-16T22:37:20.383207Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create a Spark session\u001b[39;00m\n\u001b[0;32m      9\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmazonReviewsLocalLR\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pyspark\\ml\\__init__.py:31\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     Estimator,\n\u001b[0;32m     24\u001b[0m     Model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     UnaryTransformer,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline, PipelineModel\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     classification,\n\u001b[0;32m     33\u001b[0m     clustering,\n\u001b[0;32m     34\u001b[0m     evaluation,\n\u001b[0;32m     35\u001b[0m     feature,\n\u001b[0;32m     36\u001b[0m     fpm,\n\u001b[0;32m     37\u001b[0m     image,\n\u001b[0;32m     38\u001b[0m     recommendation,\n\u001b[0;32m     39\u001b[0m     regression,\n\u001b[0;32m     40\u001b[0m     stat,\n\u001b[0;32m     41\u001b[0m     tuning,\n\u001b[0;32m     42\u001b[0m     util,\n\u001b[0;32m     43\u001b[0m     linalg,\n\u001b[0;32m     44\u001b[0m     param,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TorchDistributor\n\u001b[0;32m     48\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnaryTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorchDistributor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     71\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pyspark\\ml\\image.py:31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, NoReturn, Optional, cast\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Row, StructType, _create_row, _parse_datatype_json_string\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AmazonReviewsLocalLR\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Convert Python list of dicts to Spark DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Ensure the expected columns exist: 'text' and 'rating'\n",
    "required_columns = [\"text\", \"rating\"]\n",
    "for col_name in required_columns:\n",
    "    if col_name not in df.columns:\n",
    "        raise ValueError(f\"Expected column '{col_name}' not found in data\")\n",
    "\n",
    "# Create binary label: label=1 if rating >=3 else 0\n",
    "df = df.withColumn(\"label\", when(col(\"rating\") >= 3, 1).otherwise(0))\n",
    "\n",
    "# Filter out rows without text\n",
    "df = df.filter(col(\"text\").isNotNull())\n",
    "\n",
    "# Define text processing and feature extraction pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=20, regParam=0.001)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Fit the model\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Evaluate model performance\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = binary_evaluator.evaluate(predictions)\n",
    "\n",
    "# For multi-metric evaluation\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = multi_evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "precision = multi_evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
    "recall = multi_evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
    "f1 = multi_evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
    "\n",
    "print(\"=== Performance Metrics ===\")\n",
    "print(f\"Test AUC (Area Under ROC): {auc:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1-score: {f1:.4f}\")\n",
    "\n",
    "# Show a confusion matrix-like table\n",
    "predictions.groupBy(\"label\", \"prediction\").count().show()\n",
    "\n",
    "# Stop the Spark session (if you do not need it further)\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b672b309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.7386457473162675\n",
      "Test Accuracy: 0.8917525773195877\n",
      "Test Precision: 0.7952226591561271\n",
      "Test Recall: 0.8917525773195877\n",
      "Test F1 Score: 0.8407258630860418\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SentimentClassifier\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create a binary label: favorable (1) for ratings >= 3, not favorable (0) otherwise\n",
    "df = df.withColumn(\"label\", when(col(\"rating\") >= 3, 1).otherwise(0))\n",
    "\n",
    "# Process text column\n",
    "text_tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"text_words\")\n",
    "text_remover = StopWordsRemover(inputCol=\"text_words\", outputCol=\"filtered_text_words\")\n",
    "text_hashingTF = HashingTF(inputCol=\"filtered_text_words\", outputCol=\"text_rawFeatures\", numFeatures=10000)\n",
    "text_idf = IDF(inputCol=\"text_rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Process title column\n",
    "title_tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"title_words\")\n",
    "title_remover = StopWordsRemover(inputCol=\"title_words\", outputCol=\"filtered_title_words\")\n",
    "title_hashingTF = HashingTF(inputCol=\"filtered_title_words\", outputCol=\"title_rawFeatures\", numFeatures=5000)\n",
    "title_idf = IDF(inputCol=\"title_rawFeatures\", outputCol=\"title_features\")\n",
    "\n",
    "# Combine features from text and title\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=[\"features\", \"title_features\"], \n",
    "    outputCol=\"assembled_features\"\n",
    ")\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"assembled_features\", labelCol=\"label\", numTrees=100, maxDepth=10, seed=42)\n",
    "\n",
    "# Pipeline to chain all the stages together\n",
    "rf_pipeline = Pipeline(stages=[\n",
    "    text_tokenizer, text_remover, text_hashingTF, text_idf,\n",
    "    title_tokenizer, title_remover, title_hashingTF, title_idf,\n",
    "    feature_assembler, rf\n",
    "])\n",
    "\n",
    "# Train/test data split\n",
    "rf_train_df, rf_test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model = rf_pipeline.fit(rf_train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_predictions = rf_model.transform(rf_test_df)\n",
    "\n",
    "# Evaluate model performance and metrics\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = binary_evaluator.evaluate(rf_predictions)\n",
    "print(f\"Test AUC: {auc}\")\n",
    "\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "accuracy = multi_evaluator.evaluate(rf_predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
    "precision = multi_evaluator.evaluate(rf_predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = multi_evaluator.evaluate(rf_predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = multi_evaluator.evaluate(rf_predictions, {multi_evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Precision: {precision}\")\n",
    "print(f\"Test Recall: {recall}\")\n",
    "print(f\"Test F1 Score: {f1}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb0cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, VectorAssembler\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SentimentClassifier\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create a binary label: favorable (1) for ratings >= 3, not favorable (0) otherwise\n",
    "df = df.withColumn(\"label\", when(col(\"rating\") >= 3, 1).otherwise(0))\n",
    "\n",
    "# Process text column\n",
    "text_tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"text_words\")\n",
    "text_remover = StopWordsRemover(inputCol=\"text_words\", outputCol=\"filtered_text_words\")\n",
    "text_hashingTF = HashingTF(inputCol=\"filtered_text_words\", outputCol=\"text_rawFeatures\", numFeatures=10000)\n",
    "text_idf = IDF(inputCol=\"text_rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Process title column\n",
    "title_tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"title_words\")\n",
    "title_remover = StopWordsRemover(inputCol=\"title_words\", outputCol=\"filtered_title_words\")\n",
    "title_hashingTF = HashingTF(inputCol=\"filtered_title_words\", outputCol=\"title_rawFeatures\", numFeatures=5000)\n",
    "title_idf = IDF(inputCol=\"title_rawFeatures\", outputCol=\"title_features\")\n",
    "\n",
    "# Combine features from text and title\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=[\"features\", \"title_features\"], \n",
    "    outputCol=\"assembled_features\"\n",
    ")\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "nb = NaiveBayes(featuresCol=\"assembled_features\", labelCol=\"label\", smoothing=1.0)\n",
    "\n",
    "# Pipeline to chain all the stages together\n",
    "nb_pipeline = Pipeline(stages=[\n",
    "    text_tokenizer, text_remover, text_hashingTF, text_idf,\n",
    "    title_tokenizer, title_remover, title_hashingTF, title_idf,\n",
    "    feature_assembler, nb\n",
    "])\n",
    "\n",
    "# Train/test data split\n",
    "nb_train_df, nb_test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "nb_model = nb_pipeline.fit(nb_train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "nb_predictions = nb_model.transform(nb_test_df)\n",
    "\n",
    "# Evaluate model performance and metrics\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "auc = binary_evaluator.evaluate(nb_predictions)\n",
    "\n",
    "print(f\"Test AUC: {auc}\")\n",
    "\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = multi_evaluator.evaluate(nb_predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
    "precision = multi_evaluator.evaluate(nb_predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = multi_evaluator.evaluate(nb_predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = multi_evaluator.evaluate(nb_predictions, {multi_evaluator.metricName: \"f1\"})\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Precision: {precision}\")\n",
    "print(f\"Test Recall: {recall}\")\n",
    "print(f\"Test F1 Score: {f1}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop() # Commented out to avoid stopping the Spark session before using it for the next example, add to last algo cell\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
