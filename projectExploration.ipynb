{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T21:54:13.819038Z",
     "start_time": "2024-12-11T21:54:13.126322Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def load_jsonl_from_s3(bucket_name, file_key):\n",
    "    \"\"\"\n",
    "    Load a JSONL file from an S3 bucket using credentials from environment variables.\n",
    "\n",
    "    Parameters:\n",
    "    - bucket_name: str - Name of the S3 bucket.\n",
    "    - file_key: str - Key (path) of the JSONL file in the bucket.\n",
    "\n",
    "    Returns:\n",
    "    - List of Python dictionaries loaded from the JSONL file.\n",
    "    \"\"\"\n",
    "    # Get AWS credentials from environment variables\n",
    "    aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "    aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    region_name = os.getenv(\"AWS_REGION\")\n",
    "    \n",
    "    # Initialize an S3 client\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "        region_name=region_name\n",
    "    )\n",
    "    \n",
    "    # Retrieve the file content\n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    # Parse the JSONL content\n",
    "    data = [json.loads(line) for line in content.splitlines() if line.strip()]\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    bucket_name = \"small-reviews584\"\n",
    "    file_key = \"data/reviews_small.jsonl\"\n",
    "    \n",
    "    data = load_jsonl_from_s3(bucket_name, file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87b709a9b8828cf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T22:05:49.894667Z",
     "start_time": "2024-12-11T22:05:41.632733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.785026149187999\n",
      "Test Accuracy: 0.865979381443299\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AmazonReviewsLocalLR\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Convert Python list of dicts to Spark DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Ensure the expected columns exist: 'text' for review text and 'rating' for the star rating\n",
    "# If not present, adjust column names accordingly.\n",
    "required_columns = [\"text\", \"rating\"]\n",
    "for col_name in required_columns:\n",
    "    if col_name not in df.columns:\n",
    "        raise ValueError(f\"Expected column '{col_name}' not found in data\")\n",
    "\n",
    "# Create binary label: label=1 if rating >=3 else 0\n",
    "df = df.withColumn(\"label\", when(col(\"rating\") >= 3, 1).otherwise(0))\n",
    "\n",
    "# Filter out rows without text\n",
    "df = df.filter(col(\"text\").isNotNull())\n",
    "\n",
    "# Define text processing and feature extraction pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=20, regParam=0.001)\n",
    "\n",
    "# Create a pipeline for convenience\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Fit the model\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Evaluate model performance\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = binary_evaluator.evaluate(predictions)\n",
    "\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = multi_evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test AUC: {auc}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "# spark.stop() # Commented out to avoid stopping the Spark session before using it for the next example, add to last algo cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b672b309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.7386457473162675\n",
      "Test Accuracy: 0.8917525773195877\n",
      "Test Precision: 0.7952226591561271\n",
      "Test Recall: 0.8917525773195877\n",
      "Test F1 Score: 0.8407258630860418\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SentimentClassifier\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create a binary label: favorable (1) for ratings >= 3, not favorable (0) otherwise\n",
    "df = df.withColumn(\"label\", when(col(\"rating\") >= 3, 1).otherwise(0))\n",
    "\n",
    "# Process text column\n",
    "text_tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"text_words\")\n",
    "text_remover = StopWordsRemover(inputCol=\"text_words\", outputCol=\"filtered_text_words\")\n",
    "text_hashingTF = HashingTF(inputCol=\"filtered_text_words\", outputCol=\"text_rawFeatures\", numFeatures=10000)\n",
    "text_idf = IDF(inputCol=\"text_rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Process title column\n",
    "title_tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"title_words\")\n",
    "title_remover = StopWordsRemover(inputCol=\"title_words\", outputCol=\"filtered_title_words\")\n",
    "title_hashingTF = HashingTF(inputCol=\"filtered_title_words\", outputCol=\"title_rawFeatures\", numFeatures=5000)\n",
    "title_idf = IDF(inputCol=\"title_rawFeatures\", outputCol=\"title_features\")\n",
    "\n",
    "# Combine features from text and title\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=[\"features\", \"title_features\"], \n",
    "    outputCol=\"assembled_features\"\n",
    ")\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"assembled_features\", labelCol=\"label\", numTrees=100, maxDepth=10, seed=42)\n",
    "\n",
    "# Pipeline to chain all the stages together\n",
    "rf_pipeline = Pipeline(stages=[\n",
    "    text_tokenizer, text_remover, text_hashingTF, text_idf,\n",
    "    title_tokenizer, title_remover, title_hashingTF, title_idf,\n",
    "    feature_assembler, rf\n",
    "])\n",
    "\n",
    "# Train/test data split\n",
    "rf_train_df, rf_test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model = rf_pipeline.fit(rf_train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_predictions = rf_model.transform(rf_test_df)\n",
    "\n",
    "# Evaluate model performance and metrics\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = binary_evaluator.evaluate(rf_predictions)\n",
    "print(f\"Test AUC: {auc}\")\n",
    "\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "accuracy = multi_evaluator.evaluate(rf_predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
    "precision = multi_evaluator.evaluate(rf_predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = multi_evaluator.evaluate(rf_predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = multi_evaluator.evaluate(rf_predictions, {multi_evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Precision: {precision}\")\n",
    "print(f\"Test Recall: {recall}\")\n",
    "print(f\"Test F1 Score: {f1}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "# spark.stop() # Commented out to avoid stopping the Spark session before using it for the next example, add to last algo cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
